{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9158a1c2",
   "metadata": {},
   "source": [
    "## Objective: \n",
    "The objective of this project is to extract textual data articles from the given URL(in input Excel file i.e \"input.xlsx\" ) and perform text analysis to compute variables that are explained in Text analysis file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c16cd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "df = pd.read_excel(r\"C:\\Users\\ohkba\\OneDrive\\Documents\\Assignment Black\\input.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc8286",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6fa07",
   "metadata": {},
   "source": [
    "For each of the articles, given in the input.xlsx file, we extracted the article text and save the extracted article in a text file with URL_ID as its file name.\n",
    "While extracting text, we make sure my program extracts only the article title and the article text and It should not extract the website header, footer, or anything other than the article text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c4311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting article from https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/how-advertisement-increase-your-market-value/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/human-rights-outlook/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/ : 'NoneType' object has no attribute 'get_text'\n",
      "Error extracting article from https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/ : 'NoneType' object has no attribute 'get_text'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Attemp to extract the title \n",
    "        title = soup.find('h1', class_=\"entry-title\")\n",
    "        title_text = title.get_text(strip=True) if title else 'Title Not Found'\n",
    "        # Extract article text \n",
    "        article_text = soup.find(\"div\", class_=\"td-post-content tagdiv-type\").get_text()\n",
    "        return title_text, article_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article from {url} : {e}\") # getting error in 11 websites as div class name has been changed\n",
    "        return None, None\n",
    "    \n",
    "def save_article_to_file(url_id, title_text, article_text):\n",
    "    directory = r\"C:\\Users\\ohkba\\OneDrive\\Documents\\Assignment Black\\Text_data\"\n",
    "    filename = f\"{url_id}.txt\"\n",
    "    full_path= os.path.join(directory, filename) # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    with open(full_path, 'w', encoding='utf-8') as file:\n",
    "#         file.write(f\"Title: {title_text}\\n\\n\")\n",
    "#         file.write(f\"Article Text: {article_text}\")\n",
    "        file.write(f\"{title_text}\\n\\n\")\n",
    "        file.write(f\"{article_text}\")\n",
    "    \n",
    "    return full_path    \n",
    "\n",
    "\n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    title, article_text = extract_article_text(url)\n",
    "    article_path = save_article_to_file(url_id, title, article_text) #saving the extracted article in the text file\n",
    "\n",
    "\n",
    "# print(\"All files have been extracted and saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e2569",
   "metadata": {},
   "source": [
    "## 2. Sentimental Analysis\n",
    "Sentimental analysis is the process of determining whether a piece of writing is positive, negative, or neutral. The below Algorithm is designed for use in Financial Texts. It consists of steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65224c27",
   "metadata": {},
   "source": [
    "#### 2.1 Creating a dictionary of Positive and Negative words\n",
    "The Master Dictionary (found in the folder MasterDictionary) is used for creating a dictionary of Positive and Negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd05375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "4783\n"
     ]
    }
   ],
   "source": [
    "def read_article(path):\n",
    "    with open(path, 'r') as file:\n",
    "        article_content = file.read()\n",
    "    return article_content\n",
    "\n",
    "\n",
    "def loading_Master_dictionary(file_path):\n",
    "    words_list = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "            words_list.append(stripped_line)\n",
    "\n",
    "    return words_list\n",
    "    #print(stop_words)\n",
    "    \n",
    "    \n",
    "Positive_list = loading_Master_dictionary(r\"C:\\Users\\ohkba\\OneDrive\\Documents\\Assignment Black\\MasterDictionary\\positive-words.txt\")\n",
    "Negative_list = loading_Master_dictionary(r\"C:\\Users\\ohkba\\OneDrive\\Documents\\Assignment Black\\MasterDictionary\\negative-words.txt\")\n",
    "\n",
    "print(len(set(Positive_list)))\n",
    "print(len(set(Negative_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b01d0",
   "metadata": {},
   "source": [
    "#### 2.2 Cleaning using Stop Words Lists\n",
    "The Stop Words Lists are used to clean the text so that Sentiment Analysis can be performed by excluding the words found in Stop Words List. We will use NLTK, NLTK stands for Natural Language Toolkit. It's a comprehensive library for natural language processing (NLP) tasks in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b670ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ohkba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ohkba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import syllapy\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4336ec",
   "metadata": {},
   "source": [
    "#### Extracting Derived variables\n",
    "We convert the text into a list of tokens using the nltk tokenize module and use these tokens to calculate the 4 variables described below:\n",
    "Positive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "\n",
    "Negative Score: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number or we can assign the value of +1 for each word if found in the Negative Dictionary and then adding up all the values.\n",
    "\n",
    "Polarity Score: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
    "\n",
    "Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "Range is from -1 to +1\n",
    "\n",
    "Subjectivity Score: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula: \n",
    "\n",
    "Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "Range is from 0 to +1\n",
    "\n",
    "#### Analysis of Readability\n",
    "Analysis of Readability is calculated using the Gunning Fox index formula described below.\n",
    "\n",
    "Average Sentence Length = the number of words / the number of sentences\n",
    "\n",
    "Percentage of Complex words = the number of complex words / the number of words \n",
    "\n",
    "Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "#### Average Number of Words Per Sentence\n",
    "The formula for calculating is:\n",
    "Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "\n",
    "#### Complex Word Count\n",
    "Complex words are words in the text that contain more than two syllables.\n",
    "\n",
    "#### Word Count\n",
    "We count the total cleaned words present in the text by \n",
    "removing the stop words (using stopwords class of nltk package).\n",
    "removing any punctuations like ? ! , . from the word before counting.\n",
    "\n",
    "#### Syllable Count Per Word\n",
    "We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
    "\n",
    "#### Personal Pronouns\n",
    "To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n",
    "\n",
    "#### Average Word Length\n",
    "Average Word Length is calculated by the formula:\n",
    "Sum of the total number of characters in each word/Total number of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c55fa",
   "metadata": {},
   "source": [
    "## 3. Data Analysis\n",
    "For each of the extracted texts from the article, we will perform textual analysis and compute variables as explained above. Later we need to save the output in a excel file with URL_ID, URL and in the exact order in which the below analysis_result dictionay is created as output structure file, “Output.xlsx”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093194d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_textual_analysis(article_text, file_name):\n",
    "    \n",
    "    tokens = word_tokenize(article_text.lower())\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "#     def calculating_variables(cleaned_tokens):\n",
    "    Positive_score = sum(1 for word in cleaned_tokens if word in Positive_list)\n",
    "    Negative_score = sum(1 for word in cleaned_tokens if word in Negative_list)\n",
    "    Polarity_score = (Positive_score - Negative_score) / (Positive_score + Negative_score + 0.000001)\n",
    "    Total_words = len(cleaned_tokens)\n",
    "    Subjectivity_score = (Positive_score + Negative_score) / (Total_words + 0.000001)\n",
    "\n",
    "#         return Positive_score, Negative_score, Polarity_score, Subjectivity_score\n",
    "    \n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    sentences_count = len(sentences)\n",
    "    Average_sentence_length = Total_words/sentences_count\n",
    "    complex_words = [word for word in cleaned_tokens if syllapy.count(word) >= 3]\n",
    "    complex_word_count = len(complex_words)\n",
    "    Percentage_complex_words = complex_word_count/Total_words\n",
    "    Fog_index = 0.4 * (Average_sentence_length + Percentage_complex_words)\n",
    "    \n",
    "    Average_no_words_per_sentence = Total_words/sentences_count\n",
    "    total_chars = sum(len(word) for word in cleaned_tokens)\n",
    "    Avg_word_length = total_chars/ Total_words\n",
    "    syllables_per_word = sum(syllapy.count(word) for word in cleaned_tokens)/Total_words\n",
    "    \n",
    "    def count_personal_pronouns(cleaned_text):\n",
    "    # Define the pattern for personal pronouns\n",
    "    # Use word boundaries (\\b) to ensure we're capturing the pronouns as whole words\n",
    "    # Case insensitive matching except for \"us\" vs \"US\"\n",
    "    # For \"us\" we make sure it is either followed by a non-uppercase letter or punctuation or spaces, or at the end of the string\n",
    "        pattern = r'\\b(I|we|my|ours)\\b|\\bus\\b(?![A-Z])'\n",
    "\n",
    "    # Find all matches using the pattern\n",
    "        matches = re.findall(pattern, cleaned_text, re.IGNORECASE)\n",
    "\n",
    "    # Return the count of matches\n",
    "        return len(matches) #, matches we can also return the matches\n",
    "    \n",
    "    personal_pronouns = count_personal_pronouns(cleaned_text)\n",
    "#     file_name = str(file_name)\n",
    "    file_ID = file_name.replace(\".txt\", \"\")\n",
    "    \n",
    "    # Creating a dictionary with analysis results, will also add URL_ID for later merging the dataframes through left join on URL_ID as it's unique\n",
    "    analysis_results = {\n",
    "        \"URL_ID\": file_ID,\n",
    "        \"Positive Score\": Positive_score,\n",
    "        \"Negative Score\": Negative_score,\n",
    "        \"Polarity Score\": Polarity_score,\n",
    "        \"Subjectivity Score\": Subjectivity_score,\n",
    "        \"Avg Sentence Length\": Average_sentence_length,\n",
    "        \"Percentage Complex Words\": Percentage_complex_words,\n",
    "        \"Fog Index\": Fog_index,\n",
    "        \"Avg No of Words per Sentence\": Average_no_words_per_sentence,\n",
    "        \"Complex Word Count\": complex_word_count,\n",
    "        \"Word Count\": Total_words,\n",
    "        \"Syllables per Word\": syllables_per_word,\n",
    "        \"Personal Pronouns\": personal_pronouns,\n",
    "        \"Avg Word Length\": Avg_word_length\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd59ab6",
   "metadata": {},
   "source": [
    "The below function loading_content will go through each article text file as saved during extraction and perform the textual analysis on each content using the above perform_textual_analysis funtion and concat the new dataframe each time an analysis is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed1ac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Avg Sentence Length</th>\n",
       "      <th>Percentage Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Avg No of Words per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Syllables per Word</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Avg Word Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>5.034848</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>168</td>\n",
       "      <td>792</td>\n",
       "      <td>1.683081</td>\n",
       "      <td>2</td>\n",
       "      <td>5.380051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>66</td>\n",
       "      <td>31</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.087545</td>\n",
       "      <td>13.850000</td>\n",
       "      <td>0.328520</td>\n",
       "      <td>5.671408</td>\n",
       "      <td>13.850000</td>\n",
       "      <td>364</td>\n",
       "      <td>1108</td>\n",
       "      <td>1.937726</td>\n",
       "      <td>3</td>\n",
       "      <td>5.990975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.076739</td>\n",
       "      <td>14.631579</td>\n",
       "      <td>0.425659</td>\n",
       "      <td>6.022895</td>\n",
       "      <td>14.631579</td>\n",
       "      <td>355</td>\n",
       "      <td>834</td>\n",
       "      <td>2.238609</td>\n",
       "      <td>1</td>\n",
       "      <td>6.779376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>-0.293103</td>\n",
       "      <td>0.138756</td>\n",
       "      <td>17.787234</td>\n",
       "      <td>0.387560</td>\n",
       "      <td>7.269918</td>\n",
       "      <td>17.787234</td>\n",
       "      <td>324</td>\n",
       "      <td>836</td>\n",
       "      <td>2.084928</td>\n",
       "      <td>0</td>\n",
       "      <td>6.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.061876</td>\n",
       "      <td>12.525000</td>\n",
       "      <td>0.329341</td>\n",
       "      <td>5.141737</td>\n",
       "      <td>12.525000</td>\n",
       "      <td>165</td>\n",
       "      <td>501</td>\n",
       "      <td>1.962076</td>\n",
       "      <td>0</td>\n",
       "      <td>6.275449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID  Positive Score  Negative Score  Polarity Score  \\\n",
       "0  blackassign0001              44               6        0.760000   \n",
       "1  blackassign0002              66              31        0.360825   \n",
       "2  blackassign0003              40              24        0.250000   \n",
       "3  blackassign0004              41              75       -0.293103   \n",
       "4  blackassign0005              23               8        0.483871   \n",
       "\n",
       "   Subjectivity Score  Avg Sentence Length  Percentage Complex Words  \\\n",
       "0            0.063131            12.375000                  0.212121   \n",
       "1            0.087545            13.850000                  0.328520   \n",
       "2            0.076739            14.631579                  0.425659   \n",
       "3            0.138756            17.787234                  0.387560   \n",
       "4            0.061876            12.525000                  0.329341   \n",
       "\n",
       "   Fog Index  Avg No of Words per Sentence  Complex Word Count  Word Count  \\\n",
       "0   5.034848                     12.375000                 168         792   \n",
       "1   5.671408                     13.850000                 364        1108   \n",
       "2   6.022895                     14.631579                 355         834   \n",
       "3   7.269918                     17.787234                 324         836   \n",
       "4   5.141737                     12.525000                 165         501   \n",
       "\n",
       "   Syllables per Word  Personal Pronouns  Avg Word Length  \n",
       "0            1.683081                  2         5.380051  \n",
       "1            1.937726                  3         5.990975  \n",
       "2            2.238609                  1         6.779376  \n",
       "3            2.084928                  0         6.526316  \n",
       "4            1.962076                  0         6.275449  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# result_df = pd.DataFrame(columns=['URL_ID', 'Positive Score', 'Negative Score', 'Polarity Score',\n",
    "#        'Subjectivity Score', 'Avg Sentence Length', 'Percentage Complex Words',\n",
    "#        'Fog Index', 'Avg No of Words per Sentence', 'Complex Word Count',\n",
    "#        'Word Count', 'Syllables per Word', 'Personal Pronouns',\n",
    "#        'Avg Word Length'])\n",
    "\n",
    "\n",
    "\n",
    "def loading_content(dir_path):\n",
    "    files=os.listdir(dir_path)\n",
    "    result_df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "      \n",
    "        file_name = str(file)\n",
    "        if os.path.isfile(file_path):\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                analysis_results_dict = perform_textual_analysis(content, file_name)\n",
    "                new_df = pd.DataFrame([analysis_results_dict])\n",
    "                result_df = pd.concat([result_df, new_df], ignore_index=True)\n",
    "        \n",
    "    return result_df\n",
    "\n",
    "\n",
    "df2 = loading_content(r\"C:\\Users\\ohkba\\OneDrive\\Documents\\Assignment Black\\Text_data\")\n",
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1d15d",
   "metadata": {},
   "source": [
    "Merging the dataframe with new dataframe after performing analysis on each article so that we will get the final dataframe in the exact order as requied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aac773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Avg Sentence Length</th>\n",
       "      <th>Percentage Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Avg No of Words per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Syllables per Word</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Avg Word Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>5.034848</td>\n",
       "      <td>12.375000</td>\n",
       "      <td>168</td>\n",
       "      <td>792</td>\n",
       "      <td>1.683081</td>\n",
       "      <td>2</td>\n",
       "      <td>5.380051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>66</td>\n",
       "      <td>31</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.087545</td>\n",
       "      <td>13.850000</td>\n",
       "      <td>0.328520</td>\n",
       "      <td>5.671408</td>\n",
       "      <td>13.850000</td>\n",
       "      <td>364</td>\n",
       "      <td>1108</td>\n",
       "      <td>1.937726</td>\n",
       "      <td>3</td>\n",
       "      <td>5.990975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.076739</td>\n",
       "      <td>14.631579</td>\n",
       "      <td>0.425659</td>\n",
       "      <td>6.022895</td>\n",
       "      <td>14.631579</td>\n",
       "      <td>355</td>\n",
       "      <td>834</td>\n",
       "      <td>2.238609</td>\n",
       "      <td>1</td>\n",
       "      <td>6.779376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>-0.293103</td>\n",
       "      <td>0.138756</td>\n",
       "      <td>17.787234</td>\n",
       "      <td>0.387560</td>\n",
       "      <td>7.269918</td>\n",
       "      <td>17.787234</td>\n",
       "      <td>324</td>\n",
       "      <td>836</td>\n",
       "      <td>2.084928</td>\n",
       "      <td>0</td>\n",
       "      <td>6.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.061876</td>\n",
       "      <td>12.525000</td>\n",
       "      <td>0.329341</td>\n",
       "      <td>5.141737</td>\n",
       "      <td>12.525000</td>\n",
       "      <td>165</td>\n",
       "      <td>501</td>\n",
       "      <td>1.962076</td>\n",
       "      <td>0</td>\n",
       "      <td>6.275449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   Positive Score  Negative Score  Polarity Score  Subjectivity Score  \\\n",
       "0              44               6        0.760000            0.063131   \n",
       "1              66              31        0.360825            0.087545   \n",
       "2              40              24        0.250000            0.076739   \n",
       "3              41              75       -0.293103            0.138756   \n",
       "4              23               8        0.483871            0.061876   \n",
       "\n",
       "   Avg Sentence Length  Percentage Complex Words  Fog Index  \\\n",
       "0            12.375000                  0.212121   5.034848   \n",
       "1            13.850000                  0.328520   5.671408   \n",
       "2            14.631579                  0.425659   6.022895   \n",
       "3            17.787234                  0.387560   7.269918   \n",
       "4            12.525000                  0.329341   5.141737   \n",
       "\n",
       "   Avg No of Words per Sentence  Complex Word Count  Word Count  \\\n",
       "0                     12.375000                 168         792   \n",
       "1                     13.850000                 364        1108   \n",
       "2                     14.631579                 355         834   \n",
       "3                     17.787234                 324         836   \n",
       "4                     12.525000                 165         501   \n",
       "\n",
       "   Syllables per Word  Personal Pronouns  Avg Word Length  \n",
       "0            1.683081                  2         5.380051  \n",
       "1            1.937726                  3         5.990975  \n",
       "2            2.238609                  1         6.779376  \n",
       "3            2.084928                  0         6.526316  \n",
       "4            1.962076                  0         6.275449  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df\n",
    "merged_df = pd.merge(df3, df2, on='URL_ID', how='left')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b101a78",
   "metadata": {},
   "source": [
    "## 4. Output Data Structure\n",
    "Output Variables: \n",
    "All input variables in “Input.xlsx”,\n",
    "Positive Score,\n",
    "Negative Score,\n",
    "Polatity Score,\n",
    "Subjectivity Score,\n",
    "Avg Sentence Length,\n",
    "Percentage of Complex Words,\n",
    "Fog Index,\n",
    "Avg Number of Words Per Senentces,\n",
    "Complex Word Count,\n",
    "Word Count,\n",
    "Syllable Per Word,\n",
    "Personal Pronouns,\n",
    "Avg Word Length.\n",
    "Check out the output data structure spreadsheet for the format of your output, i.e. “Output.xlsx”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4050743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as Output.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_file_path= r\"C:\\Users\\ohkba\\OneDrive\\Documents\\Assignment Black\\output.xlsx\"\n",
    "merged_df.to_excel(excel_file_path, index=False, engine='openpyxl', sheet_name='Output')\n",
    "print(\"File saved as Output.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
